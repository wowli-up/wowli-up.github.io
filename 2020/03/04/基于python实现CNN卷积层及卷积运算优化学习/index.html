<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>基于python实现CNN卷积层及卷积运算优化学习 | Eckle的个人网站</title><meta name="description" content="基于python实现CNN卷积层及卷积运算优化学习"><meta name="keywords" content="-深度学习"><meta name="author" content="Eckle"><meta name="copyright" content="Eckle"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://hm.baidu.com"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="基于python实现CNN卷积层及卷积运算优化学习"><meta name="twitter:description" content="基于python实现CNN卷积层及卷积运算优化学习"><meta name="twitter:image" content="https://wowli-up.github.io/img/ML.png"><meta property="og:type" content="article"><meta property="og:title" content="基于python实现CNN卷积层及卷积运算优化学习"><meta property="og:url" content="https://wowli-up.github.io/2020/03/04/%E5%9F%BA%E4%BA%8Epython%E5%AE%9E%E7%8E%B0CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%8A%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E4%BC%98%E5%8C%96%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="Eckle的个人网站"><meta property="og:description" content="基于python实现CNN卷积层及卷积运算优化学习"><meta property="og:image" content="https://wowli-up.github.io/img/ML.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://wowli-up.github.io/2020/03/04/%E5%9F%BA%E4%BA%8Epython%E5%AE%9E%E7%8E%B0CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%8A%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E4%BC%98%E5%8C%96%E5%AD%A6%E4%B9%A0/"><link rel="next" title="GoogleNet 和ResNet" href="https://wowli-up.github.io/2020/03/01/GoogleNet%20%E5%92%8CResNet/"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8bb20c3fd6c323a64ea76e0ee7b26081";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://wowli-up.github/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Eckle的个人网站</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/musics/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li><li><a class="site-page" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> 游戏</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/Eckle.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">15</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">3</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">1</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/musics/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li><li><a class="site-page" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> 游戏</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#推导过程"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> 推导过程：</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#符号说明"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text"> 符号说明：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#dnn反向传播原理"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text"> DNN反向传播原理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#卷积层反向传播推导"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text"> 卷积层反向传播推导</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#前向传播"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> 前向传播：</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#im2col的实现"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text"> im2col的实现</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#代码实现"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text"> 代码实现</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#卷积层实现"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text"> 卷积层实现;</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#tensflow实现卷积层"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text"> tensflow实现卷积层</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#caffe"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text"> caffe</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#总结"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text"> 总结：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#探究如何优化卷积运算速度"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text"> 探究：如何优化卷积运算速度</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#朴素卷积naive-convolution"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text"> 朴素卷积（Naive Convolution）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#从卷积到矩阵相乘"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text"> 从卷积到矩阵相乘</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#采用矩阵相乘替换朴素卷积"><span class="toc_mobile_items-number">5.2.1.</span> <span class="toc_mobile_items-text"> 采用矩阵相乘替换朴素卷积</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#加速gemm"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text"> 加速GEMM</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#缓存"><span class="toc_mobile_items-number">5.3.1.</span> <span class="toc_mobile_items-text"> 缓存</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#平铺tiling"><span class="toc_mobile_items-number">5.3.2.</span> <span class="toc_mobile_items-text"> 平铺（Tiling）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#向量化-fma"><span class="toc_mobile_items-number">5.3.3.</span> <span class="toc_mobile_items-text"> 向量化 &amp; FMA</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#多线程处理threading"><span class="toc_mobile_items-number">5.3.4.</span> <span class="toc_mobile_items-text"> 多线程处理（Threading）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#展开unrolling"><span class="toc_mobile_items-number">5.3.5.</span> <span class="toc_mobile_items-text"> 展开（Unrolling）</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#总结-2"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text"> 总结</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#参考文章"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text"> 参考文章：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#额外笔记"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text"> 额外笔记：</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#same和valid"><span class="toc_mobile_items-number">7.1.</span> <span class="toc_mobile_items-text"> ”SAME”和“VALID”</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#推导过程"><span class="toc-number">1.</span> <span class="toc-text"> 推导过程：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#符号说明"><span class="toc-number">1.1.</span> <span class="toc-text"> 符号说明：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dnn反向传播原理"><span class="toc-number">1.2.</span> <span class="toc-text"> DNN反向传播原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积层反向传播推导"><span class="toc-number">1.3.</span> <span class="toc-text"> 卷积层反向传播推导</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#前向传播"><span class="toc-number">2.</span> <span class="toc-text"> 前向传播：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#im2col的实现"><span class="toc-number">2.1.</span> <span class="toc-text"> im2col的实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#代码实现"><span class="toc-number">3.</span> <span class="toc-text"> 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积层实现"><span class="toc-number">3.1.</span> <span class="toc-text"> 卷积层实现;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensflow实现卷积层"><span class="toc-number">3.2.</span> <span class="toc-text"> tensflow实现卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#caffe"><span class="toc-number">3.3.</span> <span class="toc-text"> caffe</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text"> 总结：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#探究如何优化卷积运算速度"><span class="toc-number">5.</span> <span class="toc-text"> 探究：如何优化卷积运算速度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#朴素卷积naive-convolution"><span class="toc-number">5.1.</span> <span class="toc-text"> 朴素卷积（Naive Convolution）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从卷积到矩阵相乘"><span class="toc-number">5.2.</span> <span class="toc-text"> 从卷积到矩阵相乘</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#采用矩阵相乘替换朴素卷积"><span class="toc-number">5.2.1.</span> <span class="toc-text"> 采用矩阵相乘替换朴素卷积</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#加速gemm"><span class="toc-number">5.3.</span> <span class="toc-text"> 加速GEMM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#缓存"><span class="toc-number">5.3.1.</span> <span class="toc-text"> 缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#平铺tiling"><span class="toc-number">5.3.2.</span> <span class="toc-text"> 平铺（Tiling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#向量化-fma"><span class="toc-number">5.3.3.</span> <span class="toc-text"> 向量化 &amp; FMA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多线程处理threading"><span class="toc-number">5.3.4.</span> <span class="toc-text"> 多线程处理（Threading）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#展开unrolling"><span class="toc-number">5.3.5.</span> <span class="toc-text"> 展开（Unrolling）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结-2"><span class="toc-number">5.4.</span> <span class="toc-text"> 总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考文章"><span class="toc-number">6.</span> <span class="toc-text"> 参考文章：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#额外笔记"><span class="toc-number">7.</span> <span class="toc-text"> 额外笔记：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#same和valid"><span class="toc-number">7.1.</span> <span class="toc-text"> ”SAME”和“VALID”</span></a></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(/img/ML.png)"><div id="post-info"><div id="post-title"><div class="posttitle">基于python实现CNN卷积层及卷积运算优化学习</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-03-04<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-03-05</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">4.7k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 18 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><h1 id="推导过程"><a class="markdownIt-Anchor" href="#推导过程"></a> 推导过程：</h1>
<h2 id="符号说明"><a class="markdownIt-Anchor" href="#符号说明"></a> 符号说明：</h2>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.5.png" data-fancybox="group" data-caption="7.5" class="fancybox"><img alt="7.5" title="7.5" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.5.png" class="lazyload"></a></p>
<h2 id="dnn反向传播原理"><a class="markdownIt-Anchor" href="#dnn反向传播原理"></a> DNN反向传播原理</h2>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.1.jpg" data-fancybox="group" data-caption="7.1" class="fancybox"><img alt="7.1" title="7.1" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.1.jpg" class="lazyload"></a></p>
<h2 id="卷积层反向传播推导"><a class="markdownIt-Anchor" href="#卷积层反向传播推导"></a> 卷积层反向传播推导</h2>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.2.jpg" data-fancybox="group" data-caption="7.2" class="fancybox"><img alt="7.2" title="7.2" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.2.jpg" class="lazyload"></a></p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.3.jpg" data-fancybox="group" data-caption="7.3" class="fancybox"><img alt="7.3" title="7.3" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.3.jpg" class="lazyload"></a></p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.4.jpg" data-fancybox="group" data-caption="7.4" class="fancybox"><img alt="7.4" title="7.4" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/7.4.jpg" class="lazyload"></a></p>
<h1 id="前向传播"><a class="markdownIt-Anchor" href="#前向传播"></a> 前向传播：</h1>
<h2 id="im2col的实现"><a class="markdownIt-Anchor" href="#im2col的实现"></a> im2col的实现</h2>
<p>im2col()：输入数据根据滤波器、步幅等展开的二维数组，每一行代表一条卷积输入数据</p>
<p>卷积就是卷积核跟图像矩阵的运算。卷积核是一个小窗口，记录的是权重。卷积核在输入图像上按步长滑动，每次操作卷积核对应区域的输入图像，将卷积核中的权值和对应的输入图像的值相乘再相加，赋给卷积核中心所对应的输出特征图的一个值。</p>
<p>im2col的作用就是优化卷积运算，如何优化呢，我们先学习一下这个函数的原理。<br>
我们假设卷积核的尺寸为2×2，输入图像尺寸为3×3.im2col$做的事情就是对于卷积核每一次要处理的小窗，将其展开到新矩阵的一行（列），新矩阵的列（行）数，就是对于一副输入图像，卷积运算的次数（卷积核滑动的次数）</p>
<hr>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/4.png" data-fancybox="group" data-caption="4" class="fancybox"><img alt="4" title="4" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/4.png" class="lazyload"></a></p>
<p>以最右侧一列为例，卷积核为2*2，所以新矩阵的列数就为4；步长为一，卷积核共滑动4次，行数就为4.</p>
<p>看到这里我就产生了一个疑问：我们把一个卷积核对应的值展开，到底应该展开为行还是列呢？卷积核的滑动先行后列还是相反？区别在哪？<br>
这其实主要取决于我们使用的框架访存的方式。计算机一次性读取相近的内存是最快的，尤其是当需要把数据送到GPU去计算的时候，这样可以节省访存的时间，以达到加速的目的。不同框架的访存机制不一样，所以会有行列相反这样的区别。在caffe框架下，im2col是将一个小窗的值展开为一行，而在matlab中则展开为列。所以说，行列的问题没有本质区别，目的都是为了在计算时读取连续的内存。<br>
这也解释了我们为什么要通过这个变化来优化卷积。如果按照数学上的步骤做卷积读取内存是不连续的，这样就会增加时间成本。同时我们注意到做卷积对应元素相乘再相加的做法跟向量内积很相似，所以通过im2col将矩阵卷积转化为矩阵乘法来实现。</p>
<h1 id="代码实现"><a class="markdownIt-Anchor" href="#代码实现"></a> 代码实现</h1>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">im2col2</span><span class="params">(input_data, fh, fw, stride=<span class="number">1</span>, pad=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">     input_data--输入数据，shape为(batch_size,Channel,Height,Width)</span></span><br><span class="line"><span class="string">     fh -- 滤波器的height 3</span></span><br><span class="line"><span class="string">     fw --滤波器的width 3</span></span><br><span class="line"><span class="string">     stride -- 步幅 1</span></span><br><span class="line"><span class="string">     pad -- 填充 1</span></span><br><span class="line"><span class="string">     Returns :</span></span><br><span class="line"><span class="string">     col -- 输入数据根据滤波器、步幅等展开的二维数组，每一行代表一条卷积数据</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    N, C, H, W = input_data.shape   <span class="string">"[20,1,28,28]"</span></span><br><span class="line"></span><br><span class="line">    out_h = (H + <span class="number">2</span> * pad - fh) // stride + <span class="number">1</span>    <span class="string">"[28]"</span></span><br><span class="line">    out_w = (W + <span class="number">2</span> * pad - fw) // stride + <span class="number">1</span>     <span class="string">"[28]"</span></span><br><span class="line"></span><br><span class="line">    img = np.pad(input_data, [(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)], <span class="string">"constant"</span>) </span><br><span class="line">     </span><br><span class="line">    <span class="string">"[30*30*1]"</span></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    col = np.zeros((N, out_h, out_w, fh * fw * C))  <span class="string">"fh * fw * C 负责存储每次参与卷积的参数"</span></span><br><span class="line">    print(col.shape)</span><br><span class="line">    <span class="comment"># 将所有维度上需要卷积的值展开成一行（列）,卷积次数为out_h*out_w*c,每次卷积内含参数量为（fh*fw*c）</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(out_h):</span><br><span class="line">        y_start = y * stride</span><br><span class="line">        y_end = y_start + fh</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> range(out_w):</span><br><span class="line">            x_start = x * stride</span><br><span class="line">            x_end = x_start + fw</span><br><span class="line">            col[:, y, x] = img[:, :, y_start:y_end, x_start:x_end].reshape(N, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    col = col.reshape(N * out_h * out_w, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> col</span><br></pre></td></tr></tbody></table></figure></div>
<p>测试：</p>
<p>输入input_data=[20,1,28,28];  fh=fw=3;采用SAME方式填充，则卷积的shape与input_data相同；</p>
<p>共卷积次数为15680次，每次卷积时input_data参与卷积的像素点数为9（卷积核为3*3）；所以col大小应为（15680，9）</p>
<p>程序输出结果：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">"测试"</span></span><br><span class="line">x=np.random.uniform(<span class="number">0</span>,<span class="number">255</span>,(<span class="number">20</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">out = im2col2(x,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">print(out.shape)  “(<span class="number">15680</span>, <span class="number">9</span>)”</span><br></pre></td></tr></tbody></table></figure></div>
<h2 id="卷积层实现"><a class="markdownIt-Anchor" href="#卷积层实现"></a> 卷积层实现;</h2>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Convolution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, W, fb, stride=<span class="number">1</span>, pad=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        W-- 滤波器权重，shape为(FN,NC,FH,FW),FN 为滤波器的个数</span></span><br><span class="line"><span class="string">        fb -- 滤波器的偏置，shape 为(1,FN)</span></span><br><span class="line"><span class="string">        stride -- 步长</span></span><br><span class="line"><span class="string">        pad -- 填充个数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.W = W</span><br><span class="line">        self.fb = fb</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.pad = pad</span><br><span class="line"></span><br><span class="line">        self.col_X = <span class="literal">None</span></span><br><span class="line">        self.X = <span class="literal">None</span></span><br><span class="line">        self.col_W = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.dW = <span class="literal">None</span></span><br><span class="line">        self.db = <span class="literal">None</span></span><br><span class="line">        self.out_shape = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#    self.out = None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        input_X-- shape为(m,nc,height,width)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.X = input_X</span><br><span class="line">        FN, NC, FH, FW = self.W.shape</span><br><span class="line"></span><br><span class="line">        m, input_nc, input_h, input_w = self.X.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment"># 将输入数据展开成二维数组，shape为（m*out_h*out_w,FH*FW*C)</span></span><br><span class="line">        self.col_X = col_X = im2col2(self.X, FH, FW, self.stride, self.pad)</span><br><span class="line">        print(<span class="string">"self.col_X.shape"</span>,self.col_X .shape)</span><br><span class="line">        <span class="comment"># 将滤波器一个个按列展开(FH*FW*C,FN)       col_W.shape 15680,9 col_w 9,20  输出 15680，20</span></span><br><span class="line">        self.col_W = col_W = self.W.reshape(FN, <span class="number">-1</span>).T</span><br><span class="line">        out = np.dot(col_X, col_W) + self.fb</span><br><span class="line"></span><br><span class="line">        out = out.T <span class="comment">#     20，15680</span></span><br><span class="line"></span><br><span class="line">        out = out.reshape(m, FN, input_h, input_w)</span><br><span class="line">        self.out_shape = out.shape</span><br><span class="line">        print(<span class="string">"out.shape"</span>, out.shape)</span><br><span class="line">        <span class="keyword">return</span> out <span class="comment">#(20, 20, 28, 28)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dz, learning_rate)</span>:</span></span><br><span class="line">        print(<span class="string">"==== Conv backbward ==== "</span>)</span><br><span class="line">        <span class="keyword">assert</span> (dz.shape == self.out_shape)</span><br><span class="line"></span><br><span class="line">        FN, NC, FH, FW = self.W.shape <span class="comment">#[20,1,28,28]</span></span><br><span class="line">        o_FN, o_NC, o_FH, o_FW = self.out_shape <span class="comment">#[20,20,28,28]</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"o_FN = {0}, o_NC = {1}, o_FH = {2}, o_FW = {3} "</span>.format(o_FN,o_NC,o_FH,o_FW))</span><br><span class="line"></span><br><span class="line">        col_dz = dz.reshape(o_NC, <span class="number">-1</span>)  <span class="comment">#col_dz  [20,15680]   dz[20, 20, 28, 28]</span></span><br><span class="line"></span><br><span class="line">        col_dz = col_dz.T</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"self.col_X.T,col_dz"</span>,self.col_X.shape,col_dz.shape)</span><br><span class="line">        self.dW = np.dot(self.col_X.T, col_dz)  <span class="comment"># [15680,9]  [15680,20]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.db = np.sum(col_dz, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.dW = self.dW.T.reshape(self.W.shape)</span><br><span class="line">        self.db = self.db.reshape(self.fb.shape)</span><br><span class="line">        print(<span class="string">"dw.shape = {0},db.shape = {1} ,self.col_W={2}"</span>.format(self.dW.shape, self.db.shape,self.col_W.shape))</span><br><span class="line">        d_col_x = np.dot(col_dz, self.col_W.T)  <span class="comment"># shape is (m*out_h*out_w,FH,FW*C)</span></span><br><span class="line">        print(<span class="string">"d_col_x.shape= "</span>, d_col_x.shape)</span><br><span class="line">        dx = col2im2(d_col_x, self.X.shape, FH, FW, stride=<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">"dx.shape= "</span>,dx.shape)</span><br><span class="line">        <span class="keyword">assert</span> (dx.shape == self.X.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新W和b</span></span><br><span class="line">        self.W = self.W - learning_rate * self.dW</span><br><span class="line">        self.fb = self.fb - learning_rate * self.db</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></tbody></table></figure></div>
<p>测试：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据载入</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">x=np.random.uniform(<span class="number">0</span>,<span class="number">255</span>,(<span class="number">20</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)) <span class="comment">#测试集采用[20,1,28,28]</span></span><br><span class="line">w=np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">20</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))   <span class="comment">#卷积核采用通道数为1的3*3的卷积核，卷积次数为20</span></span><br><span class="line">b=x=np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">1</span>,<span class="number">20</span>)) </span><br><span class="line">dz=np.random.uniform(<span class="number">0</span>,<span class="number">255</span>,(<span class="number">20</span>,<span class="number">20</span>,<span class="number">28</span>,<span class="number">28</span>))<span class="comment">#dz.shape与前向传播大小相同</span></span><br></pre></td></tr></tbody></table></figure></div>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">正向与反向传播测试</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test = Convolution(w,b)</span><br><span class="line">test.forward(x)</span><br><span class="line">test.backward(dz,<span class="number">0.01</span>)</span><br></pre></td></tr></tbody></table></figure></div>
<p>forward()内部变量输出：</p>
<p>self.col_X.shape (15680, 9)</p>
<p>out.shape (20, 20, 28, 28)</p>
<p>与设计思想相符合</p>
<p>backward()内部变量输出输出值</p>
<p>dz.shape = (20, 20, 28, 28),col_dz.shape = (20, 15680)<br>
dw.shape = (20, 1, 3, 3),db.shape = (1, 20)<br>
d_col_x.shape=  (15680, 9)<br>
dx.shape=  (20, 1, 28, 28)</p>
<h2 id="tensflow实现卷积层"><a class="markdownIt-Anchor" href="#tensflow实现卷积层"></a> tensflow实现卷积层</h2>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def conv2d_answer():</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        </span><br><span class="line">        # [N, height, width, channels] 4-D的tensors</span><br><span class="line">        #占位符</span><br><span class="line">        input_x = tf.placeholder(tf.float32, [30, 28, 28, 1], name='input_x') </span><br><span class="line">        </span><br><span class="line">        x = np.ones(shape=[30, 28, 28, 1])</span><br><span class="line"></span><br><span class="line">        #卷积核变量</span><br><span class="line">        filter_w = tf.get_variable(</span><br><span class="line">            'w', initializer=tf.truncated_normal(shape=[3, 3, 1, 20])</span><br><span class="line">        )</span><br><span class="line">        filter_b = tf.get_variable(</span><br><span class="line">            'b', initializer=tf.zeros(20)</span><br><span class="line">        )</span><br><span class="line">        strides = [1, 2, 2, 1]  # 标准的写法。</span><br><span class="line">        pad = 'VALID'</span><br><span class="line">    </span><br><span class="line">        conv_output = tf.nn.conv2d(</span><br><span class="line">            input=input_x, filter=filter_w, strides=strides, padding=pad</span><br><span class="line">        )</span><br><span class="line">        print(conv_output.get_shape())</span><br><span class="line">        conv_output = conv_output + filter_b</span><br><span class="line">        # print(conv_output)</span><br><span class="line">        # fixme 高级api</span><br><span class="line">        conv_output1 = tf.layers.conv2d(</span><br><span class="line">            inputs=input_x, kernel_size=7, filters=20, strides=2, padding='valid',</span><br><span class="line">            use_bias=True</span><br><span class="line">        )</span><br><span class="line">        print(conv_output1.get_shape())</span><br><span class="line">        # with tf.Session() as sess:</span><br><span class="line">        #     sess.run(tf.global_variables_initializer())</span><br><span class="line">        #     print(sess.run(conv_output, feed_dict={input_x: x}))</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    conv2d_answer()</span><br></pre></td></tr></tbody></table></figure></div>
<p>tensflow卷积层通过函数 tf.nn.conv2d 或者tf.layers.conv2d实现，</p>
<pre><code>"""
    tf.nn.conv2d(input,    # 卷积的输入，必须是一个4-D tensor对象
        filter,            # 滤波器
        strides,           # 步幅
        padding,           # 填充方式  string:  'SAME'   or  'VALID'
        data_format="NHWC",   # 对输入数据格式的要求，[N, height, width, channels]； 也可以是另一种格式："NCHW"
        dilations=[1, 1, 1, 1], 
        name=None)
    """
</code></pre>
<h2 id="caffe"><a class="markdownIt-Anchor" href="#caffe"></a> caffe</h2>
<p>在Caffe中是使用src/caffe/util/im2col.cu中的im2col和col2im来完成矩阵的变形和还原操作，即为上方python实现的代码，将卷积运算转化为矩阵相乘，提升了运算速度。</p>
<h1 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结：</h1>
<p>相比于现有框架，我的卷积运算部分还有待优化。实现了以矩阵相乘实现卷积运算，但是内存利用率低，可以用加速GEMM进行提高。</p>
<h1 id="探究如何优化卷积运算速度"><a class="markdownIt-Anchor" href="#探究如何优化卷积运算速度"></a> 探究：如何优化卷积运算速度</h1>
<p>提升卷积层运算速度，主要在于提高卷积的计算速度。</p>
<h2 id="朴素卷积naive-convolution"><a class="markdownIt-Anchor" href="#朴素卷积naive-convolution"></a> <strong>朴素卷积（Naive Convolution）</strong></h2>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">'''Convolve `input` with `kernel` to generate `output`    </span><br><span class="line">input.shape = [input_channels, input_height, input_width]    </span><br><span class="line">kernel.shape = [num_filters, input_channels, kernel_height, kernel_width]    </span><br><span class="line">output.shape = [num_filters, output_height, output_width]</span><br><span class="line">'''</span><br><span class="line">for filter in 0..num_filters    </span><br><span class="line">    for channel in 0..input_channels        </span><br><span class="line">        for out_h in 0..output_height            </span><br><span class="line">            for out_w in 0..output_width                </span><br><span class="line">                for k_h in 0..kernel_height    </span><br><span class="line">                   for k_w in 0..kernel_width   </span><br><span class="line">                    output[filter, channel, out_h, out_h] +=   </span><br><span class="line">                    kernel[filter, channel, k_h, k_w] *    </span><br><span class="line">                    input[channel, out_h + k_h, out_w + k_w]</span><br></pre></td></tr></tbody></table></figure></div>
<p>涉及到了6个for嵌套循环，最内的循环进行了两次浮点运算（乘和加）。对于实验所使用的卷积层规模，它执行了8516万次，即该卷积需要1.7亿次浮点运算（170MFLOPs）。</p>
<p>内存访问同样需要时间：无法快速获取数据则无法快速处理数据。上述高度嵌套的for-loop使得数据访问非常艰难，从而无法充分利用缓存。</p>
<p>探究问题：如何访问正在处理的数据，以及这与数据存储方式有何关联。</p>
<p>逻辑上我们将矩阵/图像/张量看作是多维度的，但实际上它们存储在线性、一维的计算机内存中。我们必须定义一个惯例，来规定如何将多个维度展开到线性一维存储空间中，反之亦然。</p>
<p>大部分现代深度学习库使用行主序作为存储顺序。这意味着同一行的连续元素被存储在相邻位置。对于多维度而言，行主序通常意味着：在线性扫描内存时第一个维度的变化速度最慢。</p>
<h2 id="从卷积到矩阵相乘"><a class="markdownIt-Anchor" href="#从卷积到矩阵相乘"></a> <strong>从卷积到矩阵相乘</strong></h2>
<h3 id="采用矩阵相乘替换朴素卷积"><a class="markdownIt-Anchor" href="#采用矩阵相乘替换朴素卷积"></a> 采用矩阵相乘替换朴素卷积</h3>
<p>卷积是滤波器和输入图像块（patch）的点乘。如果我们将滤波器展开为2-D矩阵，将输入块展开为另一个2-D矩阵，则将两个矩阵相乘可以得到同样的数字。将图像块展开为矩阵的过程叫做im2col（image to column）。我们将图像重新排列为矩阵的列，每个列对应一个输入块，卷积滤波器就应用于这些输入块上。</p>
<p>在现实中，不同图像块之间通常会有重叠，因而im2col可能导致内存重叠。生成im2col 缓冲（im2col buffer）和过多内存（inflated memory）所花费的时间必须通过GEMM(矩阵乘优化)实现的加速来抵消。</p>
<h2 id="加速gemm"><a class="markdownIt-Anchor" href="#加速gemm"></a> <strong>加速GEMM</strong></h2>
<h3 id="缓存"><a class="markdownIt-Anchor" href="#缓存"></a> <strong>缓存</strong></h3>
<p>RAM是大的存储空间，但速度较慢。CPU缓存的速度要快得多，但其规模较小，因此恰当地使用CPU缓存至关重要。但是并不存在明确的指令：「将该数据加载到缓存」。该过程是由CPU自动管理的。</p>
<p>每一次从主内存中获取数据时，CPU会将该数据及其邻近的数据加载到缓存中，以便利用访问局部性（locality of reference）。</p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.1.jpg" data-fancybox="group" data-caption="1.1" class="fancybox"><img alt="1.1" title="1.1" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.1.jpg" class="lazyload"></a></p>
<p>你应该首先注意到我们访问数据的模式。我们按照下图A的形式逐行遍历数据，按照下图B的形式逐列遍历数据。</p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.2.jpg" data-fancybox="group" data-caption="1.2" class="fancybox"><img alt="1.2" title="1.2" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.2.jpg" class="lazyload"></a></p>
<p>它们的存储也是行优先的，因此一旦我们找到 A[i, k]，则它在该行中的下一个元素A[i, k+1]已经被缓存了。接下来我们来看B中发生了什么：</p>
<ul>
<li>列的下一个元素并未出现在缓存中，即出现了缓存缺失（cache miss）。这时尽管获取到了数据，CPU也出现了一次停顿。</li>
<li>获取数据后，缓存同时也被 B 中同一行的其他元素填满。我们实际上并不会使用到它们，因此它们很快就会被删除。多次迭代后，当我们需要那些元素时，我们将再次获取它们。我们在用实际上不需要的值污染缓存。</li>
</ul>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.3.jpg" data-fancybox="group" data-caption="1.3" class="fancybox"><img alt="1.3" title="1.3" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.3.jpg" class="lazyload"></a></p>
<p>我们需要重新修改loop，以充分利用缓存能力。如果数据被读取，则我们要使用它。这就是我们所做的第一项改变：循环重排序（loop reordering）。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">for i in 0..M:    </span><br><span class="line">    for j in 0..N:        </span><br><span class="line">        for k in 0..K:            </span><br><span class="line">            C[i, j] += A[i, k] * B[k, j]</span><br></pre></td></tr></tbody></table></figure></div>
<p>将i,j,k 循环重新排序为 i,k,j：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">for i in 0..M:    </span><br><span class="line">    for k in 0..K:        </span><br><span class="line">        for j in 0..N:</span><br></pre></td></tr></tbody></table></figure></div>
<p>乘/加的顺序对结果没有影响。而遍历顺序则变成了如下状态：</p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.4.jpg" data-fancybox="group" data-caption="1.4" class="fancybox"><img alt="1.4" title="1.4" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.4.jpg" class="lazyload"></a></p>
<p>速度大大提升。</p>
<h3 id="平铺tiling"><a class="markdownIt-Anchor" href="#平铺tiling"></a> <strong>平铺（Tiling）</strong></h3>
<p>要想进一步改进重排序，我们需要考虑另一个缓存问题。</p>
<p>对于A中的每一行，我们针对B中所有列进行循环。而对于 B 中的每一步，我们会在缓存中加载一些新的列，去除一些旧的列。当到达A的下一行时，我们仍旧重新从第一列开始循环。我们不断在缓存中添加和删除同样的数据，即缓存颠簸（cache thrashing）。</p>
<p>如果所有数据均适应缓存，则颠簸不会出现。如果我们处理的是小型矩阵，则它们会舒适地待在缓存里，不用经历重复的驱逐。庆幸的是，我们可以将矩阵相乘分解为子矩阵。要想计算 C 的r×c平铺，我们仅需要A的r行和B的c列。接下来，我们将 C 分解为6x16的平铺：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">C(x, y) += A(k, y) * B(x, k);</span><br><span class="line"></span><br><span class="line">C.update().tile(x, y, xo, yo, xi, yi, 6, 16)</span><br><span class="line">/*in pseudocode:for xo in 0..N/16:    </span><br><span class="line">for yo in 0..M/6:        </span><br><span class="line">    for yi in 6:            </span><br><span class="line">        for xi in 0..16:                </span><br><span class="line">            for k in 0..K:                    </span><br><span class="line">                C(...) = ...*/</span><br></pre></td></tr></tbody></table></figure></div>
<p>我们将x,y 维度分解为外侧的xo,yo和内侧的xi,yi。我们将为该6x16 block优化micro-kernel（即xi,yi），然后在所有block上运行micro-kernel（通过xo,yo进行迭代）。</p>
<h3 id="向量化-fma"><a class="markdownIt-Anchor" href="#向量化-fma"></a> <strong>向量化 & FMA</strong></h3>
<p>大部分现代CPU支持SIMD（Single Instruction Multiple Data，单指令流多数据流）。在同一个CPU循环中，SIMD可在多个值上同时执行相同的运算/指令（如加、乘等）。如果我们在4个数据点上同时运行SIMD指令，就会直接实现4倍的加速。</p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.5.jpg" data-fancybox="group" data-caption="1.5" class="fancybox"><img alt="1.5" title="1.5" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/cnn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/1.5.jpg" class="lazyload"></a></p>
<p>因此，当我们计算处理器的峰值速度时，我们其实有些作弊，把该向量化性能作为峰值性能。对于向量等数据而言，SIMD用处多多，在处理此类数据时，我们必须对每一个向量元素执行同样的指令。但是我们仍然需要设计计算核心，以充分利用它。<br>
计算峰值FLOPs时，我们所使用的第二个技巧是FMA（Fused Multiply-Add）。尽管乘和加是两种独立的浮点运算，但它们非常常见，有些专用硬件单元可以将二者融合为一，作为单个指令来执行。编译器通常会管理FMA的使用。<br>
在英特尔CPU上，我们可以使用SIMD（AVX & SSE）在单个指令中处理多达8个浮点数。编译器优化通常能够独自识别向量化的时机，但是我们需要掌控向量化以确保无误。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">C.update().tile(x, y, xo, yo, xi, yi, 6, 16).reorder(xi, yi, k, xo, yo).vectorize(xi, 8)</span><br><span class="line">/*in pseudocode:for xo in 0..N/16:    </span><br><span class="line">for yo in 0..M/6:        </span><br><span class="line">    for k in 0..K:            </span><br><span class="line">        for yi in 6:                </span><br><span class="line">            for vectorized xi in 0..16:                    </span><br><span class="line">                C(...) = ...*/</span><br></pre></td></tr></tbody></table></figure></div>
<h3 id="多线程处理threading"><a class="markdownIt-Anchor" href="#多线程处理threading"></a> <strong>多线程处理（Threading）</strong></h3>
<p>到现在为止，我们仅使用了一个CPU内核。我们拥有多个内核，每个内核可同时执行多个指令。一个程序可被分割为多个线程，每个线程在单独的内核上运行。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">C.update().tile(x, y, xo, yo, xi, yi, 6, 16).reorder(xi, yi, k, xo, yo).vectorize(xi, 8).parallel(yo)</span><br><span class="line">/*in pseudocode:for xo in 0..N/16 in steps of 16:    </span><br><span class="line">for parallel yo in steps of 6:        </span><br><span class="line">    for k in 0..K:            </span><br><span class="line">        for yi in 6:                </span><br><span class="line">            for vectorized xi in 0..16 in steps of 8:                    </span><br><span class="line">                C(...) = ...*/</span><br></pre></td></tr></tbody></table></figure></div>
<p>你可能注意到，对于非常小的规模而言，性能反而下降了。这是因为工作负载很小，线程花费较少的时间来处理工作负载，而花费较多的时间同步其他线程。多线程处理存在大量此类问题。</p>
<h3 id="展开unrolling"><a class="markdownIt-Anchor" href="#展开unrolling"></a> <strong>展开（Unrolling）</strong></h3>
<p>循环使我们避免重复写同样代码的痛苦，但同时它也引入了一些额外的工作，如检查循环终止、更新循环计数器、指针运算等。如果手动写出重复的循环语句并展开循环，我们就可以减少这一开销。例如，不对1个语句执行8次迭代，而是对4个语句执行2次迭代。</p>
<p>这种看似微不足道的开销实际上是很重要的，最初意识到这一点时我很惊讶。尽管这些循环操作可能「成本低廉」，但它们肯定不是免费的。每次迭代2-3个额外指令的成本会很快累积起来，因为此处的迭代次数是数百万。随着循环开销越来越小，这种优势也在不断减小。</p>
<p>展开是几乎完全被编译器负责的另一种优化方式，除了我们想要更多掌控的micro-kernel。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">C.update().tile(x, y, xo, yo, xi, yi, 6, 16).reorder(xi, yi, k, xo, yo).vectorize(xi, 8).unroll(xi).unroll(yi)</span><br><span class="line">/*in pseudocode:for xo in 0..N/16:    </span><br><span class="line">for parallel yo:        </span><br><span class="line">    for k in 0..K:            </span><br><span class="line">        C(xi:xi+8, yi+0)            </span><br><span class="line">        C(xi:xi+8, yi+1)            </span><br><span class="line">        ...            </span><br><span class="line">        C(xi:xi+8, yi+5)            </span><br><span class="line">        C(xi+8:xi+16, yi+0)            </span><br><span class="line">        C(xi+8:xi+16, yi+1)            </span><br><span class="line">        ...            </span><br><span class="line">        C(xi+8:xi+16, yi+5)*/</span><br></pre></td></tr></tbody></table></figure></div>
<p>现在我们可以将速度提升到接近60 GFLOP/s。</p>
<h2 id="总结-2"><a class="markdownIt-Anchor" href="#总结-2"></a> <strong>总结</strong></h2>
<p>上述步骤涵盖一些性能加速最常用的变换。它们通常以不同方式组合，从而得到更加复杂的调度策略来计算同样的任务。</p>
<p>下面就是用Halide语言写的一个调度策略：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">matrix_mul(x, y) += A(k, y) * B(x, k);    </span><br><span class="line">out(x, y) = matrix_mul(x, y);</span><br><span class="line"></span><br><span class="line">out.tile(x, y, xi, yi, 24, 32)        </span><br><span class="line">  .fuse(x, y, xy).parallel(xy)        </span><br><span class="line">  .split(yi, yi, yii, 4)        </span><br><span class="line">  .vectorize(xi, 8)        </span><br><span class="line">  .unroll(xi)        </span><br><span class="line">  .unroll(yii);</span><br><span class="line"></span><br><span class="line"> matrix_mul.compute_at(out, yi)        </span><br><span class="line">   .vectorize(x, 8).unroll(y);</span><br><span class="line"></span><br><span class="line"> matrix_mul.update(0)        </span><br><span class="line">   .reorder(x, y, k)        </span><br><span class="line">   .vectorize(x, 8)        </span><br><span class="line">   .unroll(x)        </span><br><span class="line">   .unroll(y)        </span><br><span class="line">   .unroll(k, 2);</span><br></pre></td></tr></tbody></table></figure></div>
<ol>
<li>将out分解为32x24的平铺，然后将每个平铺进一步分解为8x24的子平铺。</li>
<li>使用类似的重排序、向量化和展开，在临时缓冲区（matrix_mul）计算8x24 matmul。</li>
<li>使用向量化、展开等方法将临时缓冲区matrix_mul 复制回out。</li>
<li>在全部32x24平铺上并行化这一过程</li>
</ol>
<h1 id="参考文章"><a class="markdownIt-Anchor" href="#参考文章"></a> 参考文章：</h1>
<p><a href="tps://zhuanlan.zhihu.com/p/66958390">通用矩阵乘（GEMM）优化与卷积计算</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/85344625" target="_blank" rel="noopener">如何实现高速卷积？深度学习库使用了这些「黑魔法</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/79584889" target="_blank" rel="noopener">手推DNN，CNN池化层，卷积层反向传播</a></p>
<h1 id="额外笔记"><a class="markdownIt-Anchor" href="#额外笔记"></a> 额外笔记：</h1>
<h2 id="same和valid"><a class="markdownIt-Anchor" href="#same和valid"></a> ”SAME”和“VALID”</h2>
<p>卷积之后的尺寸大小计算公式为：</p>
<ul>
<li>input_x  (FN,FC,FH,FW)</li>
<li>滤波器  Filter大小  (FN,FC,FH,FW)</li>
<li>Height与width  步长strides   S</li>
<li>Padding size P</li>
<li>求输出的shape: (FN,FC,FH,FW)</li>
</ul>
<p>我们可以得出输出大小</p>
<p>new_height = (input_height - filter_height + 2 * P)/S + 1<br>
new_width = (input_width - filter_width + 2 * P)/S + 1</p>
<p>在实际操作时，我们还会碰到 **padding的两种方式 “SAME” 和 “VALID”，padding = “SAME”时，会在图像的周围填 “0”，padding = “VALID”则不需要，即 P=0。**一般会选“SAME”，以来减缓图像变小的速度，二来防止边界信息丢失</p>
<ol>
<li>padding = “VALID”：  P=0</li>
<li>padding = “SAME”：  kernel_size=1时，P=0；kernel_size=3时，P=1；kernel_size=5时，P=3，以此类推。</li>
</ol>
</body></html></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Eckle</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wowli-up.github.io/2020/03/04/%E5%9F%BA%E4%BA%8Epython%E5%AE%9E%E7%8E%B0CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%8A%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E4%BC%98%E5%8C%96%E5%AD%A6%E4%B9%A0/">https://wowli-up.github.io/2020/03/04/%E5%9F%BA%E4%BA%8Epython%E5%AE%9E%E7%8E%B0CNN%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%8A%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E4%BC%98%E5%8C%96%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wowli-up.github.io">Eckle的个人网站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">-深度学习    </a></div><div class="post_share"><div class="social-share" data-image="/img/ML.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/03/01/GoogleNet%20%E5%92%8CResNet/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>GoogleNet 和ResNet</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'O2mOsDYD5Hx3vwHP4i02Vahz-gzGzoHsz',
  appKey:'kkskDORAC7MO6x0aNw0wMQJp',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" style="background-image: url(/img/ML.png)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By Eckle</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://wowli-up.github.io/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="false"></script><script id="ribbon_piao" mobile="true" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/piao.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script src="https://cdn.jsdelivr.net/npm/activate-power-mode/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true; 
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/click_heart.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>