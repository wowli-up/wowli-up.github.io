<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>tensflow | Eckle的个人网站</title><meta name="description" content="tensflow"><meta name="author" content="Eckle"><meta name="copyright" content="Eckle"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://hm.baidu.com"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="tensflow"><meta name="twitter:description" content="tensflow"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta property="og:type" content="article"><meta property="og:title" content="tensflow"><meta property="og:url" content="https://wowli-up.github.io/2020/01/17/tensflow/"><meta property="og:site_name" content="Eckle的个人网站"><meta property="og:description" content="tensflow"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="canonical" href="https://wowli-up.github.io/2020/01/17/tensflow/"><link rel="prev" title="cnn" href="https://wowli-up.github.io/2020/02/03/cnn/"><link rel="next" title="感知器模型" href="https://wowli-up.github.io/2020/01/13/%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A8%A1%E5%9E%8B/"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8bb20c3fd6c323a64ea76e0ee7b26081";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://wowli-up.github/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"title":"Snackbar.bookmark.title","message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">Eckle的个人网站</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/musics/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li><li><a class="site-page" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> 游戏</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/Eckle.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">22</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">5</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/musics/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> 书籍</span></a></li><li><a class="site-page" href="/games/"><i class="fa-fw fa fa-gamepad"></i><span> 游戏</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#d-and-e-will-only-run-after-a-b-c-have-executed"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> “d” and “e” will only run after ‘’’a‘’ ‘’b’’ ‘’c’‘ have executed</span></a></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#d-and-e-will-only-run-after-a-b-c-have-executed"><span class="toc-number">1.</span> <span class="toc-text"> “d” and “e” will only run after ‘’’a‘’ ‘’b’’ ‘’c’‘ have executed</span></a></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png)"><div id="post-info"><div id="post-title"><div class="posttitle">tensflow</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-01-17<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-02-03</time><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">7.5k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 29 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><p>TensorFlow变量作用域 TensorFlow™ 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。 其命名来源于本身的原理，Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算.Tensorflow运行过程就是张量从图的一端流动到另一端的计算过程。张量从图中流过的直观图像是其取名为“TensorFlow”的原因。</p>
<p>TensorFlow的关键点是：“Data Flow Graphs”，表示TensorFlow是一种基于图的计算框架，其中节点（Nodes）在图中表示数学操作，线（Edges）则表示在节点间相互联系的多维数据数组，即张量（Tensor），这种基于流的架构让TensorFlow具有非常高的灵活性，该灵活性也让TensorFlow框架可以在多个平台上进行计算，例如：台式计算机、服务器、移动设备等。<br>
备注：TensorFlow的开发过程中，重点在于构建执行流图。</p>
<p>What is Data Flow Graphs?</p>
<p>数据流图使用节点（Node）和线（Edges）的有向图描述数学计算；节点一般用来表示施加的数学操作，也可以表示数据输入(feed in)的起点和输出(push out)的终点，或者是读取/写入持久变量(persistent variable)的终点。线表示的是节点之间的输入/输出关系，这些线可以输运“size可动态调整”的多维数组，即张量(Tensor)。</p>
<p>一旦输入端的所有张量准备好，节点将被分配到各种计算设备完成异步并行地执行运算。<a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tens/1_ys.png" data-fancybox="group" data-caption="1_ys" class="fancybox"><img alt="1_ys" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tens/1_ys.png" class="lazyload" title="1_ys"></a></p>
<p>TensorFlow基本概念:</p>
<p>图（Graph）：图描述了计算的过程，TensorFlow使用图来表示计算任务。</p>
<p>张量（Tensor）：TensorFlow使用tensor表示数据。每个Tensor是一个类型化的多维数组。</p>
<p>操作（op）：图中的节点被称为op（opearation的缩写），一个op获得/输入0个或多个Tensor，执行计算，产生0个或多个Tensor。</p>
<p>变量（Variable）：运行过程中可以被改变，用于维护状态。</p>
<p>会话（Session）：图必须在称之为“会话”的上下文中执行。会话将图的op分发到诸如CPU或GPU之类的设备上执行。</p>
<p>TensorFlow的边即有两种连接关系：<br>
数据依赖<br>
控制依赖<br>
实线边表示数据依赖，代表数据，即张量。任意维度的数据统称为张量。在机器学习算法中，张量在数据流图中从前往后流动一遍就完成一次前向传播，而残差从后向前流动一遍就完成一次反向传播。<br>
虚线边表示控制依赖，可以用于控制操作的运行，这被用来确保happens-before关系，这类边上没有数据流过，但源节点必须在目的节点开始执行前完成。</p>
<p>数据属性：</p>
<table>
<thead>
<tr>
<th><strong>数据类型</strong></th>
<th><strong>Python****类型</strong></th>
<th><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>DT_FLOAT</td>
<td>tf.float32</td>
<td>32位浮点型</td>
</tr>
<tr>
<td>DT_DOUBLE</td>
<td>tf.float64</td>
<td>64位浮点型</td>
</tr>
<tr>
<td>DT_INT64</td>
<td>tf.int64</td>
<td>64位有符号整型</td>
</tr>
<tr>
<td>DT_INT32</td>
<td>tf.int32</td>
<td>32位有符号整型</td>
</tr>
<tr>
<td>DT_INT16</td>
<td>tf.int16</td>
<td>16位有符号整型</td>
</tr>
<tr>
<td>DT_INT8</td>
<td>tf.int8</td>
<td>8位有符号整型</td>
</tr>
<tr>
<td>DT_UINT8</td>
<td>tf.uint8</td>
<td>8位无符号整型</td>
</tr>
<tr>
<td>DT_STRING</td>
<td>tf.string</td>
<td>可变长度的字节数组，每一个张量元素都是一个字节数组</td>
</tr>
<tr>
<td>DT_BOOL</td>
<td>tf.bool</td>
<td>布尔型</td>
</tr>
<tr>
<td>DT_COMPLEX64</td>
<td>tf.complex64</td>
<td>由两个32位浮点数组成的复数：实数和虚数</td>
</tr>
</tbody>
</table>
<p>节点：</p>
<p>节点又称为算子，它代表一个操作，一般用来表示施加的数字运算，也可以表示数据输入的起点以及输出的重点，或者是读取/写出持久化变量的终点。</p>
<table>
<thead>
<tr>
<th><strong>类别</strong></th>
<th><strong>示例</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>数学运算操作</td>
<td>Add、Subtract、Multiply、Div、Exp、Log、Greater、Less、Equal……</td>
</tr>
<tr>
<td>数组运算操作</td>
<td>Concat, Slice, Split, Constant, Rank, Shape, Shuffle……</td>
</tr>
<tr>
<td>矩阵运算操作</td>
<td>MatMul, MatrixInverse, MatrixDeterminant……</td>
</tr>
<tr>
<td>有状态的操作</td>
<td>Variable、Assign、AssignAdd……</td>
</tr>
<tr>
<td>神经网络构建操作</td>
<td>SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool……</td>
</tr>
<tr>
<td>检查点操作</td>
<td>Save, Restore……</td>
</tr>
<tr>
<td>队列和同步操作</td>
<td>Enqueue, Dequeue, MutexAcquire, MutexRelease……</td>
</tr>
<tr>
<td>控制张量流动的操作</td>
<td>Merge, Switch, Enter, Leave, NextIteration……</td>
</tr>
</tbody>
</table>
<p>使用TensorFlow必须理解下列概念：<br>
使用图(graph)来表示计算任务；<br>
在会话(session)的上下文中执行图；<br>
使用tensor表示数据；<br>
通过变量(Variable)来维护状态 ；<br>
使用feed和fetch可以为任意的操作(Operation/op)赋值或者从其中获取数据。</p>
<p>TensorFlow程序结构:</p>
<p>TensorFlow的程序一般分为两个阶段：构建阶段和执行阶段；<br>
构建阶段：op的执行步骤被描述称为一个图，然后使用TensorFlow提供的API构建这个图。<br>
执行阶段：将构建好的执行图(Operation Graph)在给定的会话中执行，并得到执行结果。</p>
<p>TensorFlow图：</p>
<p>不使用默认图(Graph)，使用多个图来进行编程；但是注意：操作必须属于同一个图，不同图中的节点不能相连。</p>
<p>TensorFlow会话:</p>
<p>当执行图构建完成后，才能给启动图，进入到执行阶段；启动图的第一步就是创建一个Session对象，如果无任何参数的情况下，会话构造器将启动默认图。</p>
<p>tf.Session在构建会话的时候，如果不给定任何参数，那么构建出来Session对应的内部的Graph其实就是默认Graph，不过我们可以通过参数给定具体对应的是那一个Graph以及当前Session对应的配合参数。Session的构造主  要有三个参数，作用如下：<br>
target：给定连接的url，只有当分布式运行的时候需要给定(后面分布式运行讲)；<br>
graph：给定当前Session对应的图，默认为TensorFlow中的默认图；<br>
config：给定当前Session的相关参数，参数详见：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto%E4%B8%AD%E7%9A%84%5BConfigProto%5D" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto中的[ConfigProto]</a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">tf.Session(target='', graph=None, config=None)</span><br><span class="line">    target: 给定连接的url，只有当分布式运行的时候需要给定。</span><br><span class="line">    graph:  调用哪张图，如果不给定，则调用默认图。</span><br><span class="line">    config: 会话的配置文件。</span><br><span class="line">"""</span><br></pre></td></tr></tbody></table></figure></div>
<p>通过Session的config参数可以对TensorFlow的应用的执行进行一些优化调整，主要涉及到的参数如下：</p>
<table>
<thead>
<tr>
<th><strong>属性</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>gpu_options</td>
<td>GPU相关参数，主要参数：per_process_gpu_memory_fraction和allow_growth</td>
</tr>
<tr>
<td>allow_soft_placement</td>
<td>是否允许动态使用CPU和GPU，默认为False；当我们的安装方式为GPU的时候，建议该参数设置为True，因为TensorFlow中的部分op只能在CPU上运行。</td>
</tr>
<tr>
<td>log_device_placement</td>
<td>是否打印日志，默认为False，不打印日志</td>
</tr>
<tr>
<td>graph_options</td>
<td>Graph优化相关参数，一般不需要给定，默认即可，主要参数：optimizer_options(do_common_subexpression_elimination、do_constant_folding和opt_level)</td>
</tr>
</tbody>
</table>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">"""</span><br><span class="line">gpu_options 参数</span><br><span class="line">    per_process_gpu_memory_fraction 浮点数[0, 1.0]， 表示限制该gpu设备显存使用的百分比</span><br><span class="line">    allow_growth  bool值，不预先分配使用整个gpu显存计算，而是从小到大按需增长。</span><br><span class="line">"""</span><br></pre></td></tr></tbody></table></figure></div>
<p>在TensorFlow中，除了可以使用Session表示这个会话外，还可以通过InteractiveSession来表示会话，InteractiveSession的意思是：交互式会话，使用交互式会话可以降低代码的复杂度，使用Tensor.eval()或者Operation.run()来代替Session.run()方法，这样可以避免一个变量来维持会话；备注：Session也可以使用Tensor.eval()和Operation.run()获取数据/执行操作(只要明确当前会话)。</p>
<p>Tensor张量：</p>
<p>TensorFlow使用Tensor数据结构来代表所有数据，计算图中，操作间传递的数据都是Tensor。Tensor可以看作是一个n维的数组或者列表，一个Tensor主要由一个静态数据类型和动态类型的维数(Rank、Shape)组成。Tensor可以在图中的节点之间流通。</p>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tens/2.png" data-fancybox="group" data-caption="2" class="fancybox"><img alt="2" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tens/2.png" class="lazyload" title="2"></a></p>
<p>TensorFlow变量：</p>
<p>变量(Variables)是维护图执行过程中的状态信息。在训练模型过程中，可以通过变量来存储和更新参数。变量包含张量(Tensor)存放于内存的缓存区。建模的时候变量必须被明确的初始化，模型训练后变量必须被存储到磁盘。这些变量的值可以在之后的模型训练和分析中被加载。</p>
<p>在构建变量的时候，必须将一个张量或者可以转化为张量的Python对象作为初始值传入构造函数Variable中。</p>
<p>占位符：</p>
<p>可以使用 shape=[None, 3]， None使用类似于numpy。</p>
<p>input_x = tf.placeholder(dtype=tf.float32, shape=[None, 3], name=‘input_x’)</p>
<p>y_hat_, y_hat1_ = sess.run(<br>
fetches=[y_hat, y_hat1], feed_dict={input_x: data2, input_c: 10.0})</p>
<p>以字典方式通过feed_dict传入</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">"""</span><br><span class="line">使用tensorflow 实现简单的 线性回归  y = np.dot(x, W) + b</span><br><span class="line">"""</span><br><span class="line"></span><br><span class="line">def f1():</span><br><span class="line">    """</span><br><span class="line">    先使用常量进行构建，展示大致的业务逻辑</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    # 一、建图</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、创建模型输入</span><br><span class="line">        input_x = tf.constant(</span><br><span class="line">            value=[[1,2,3],</span><br><span class="line">                   [2,3,4],</span><br><span class="line">                   [12,34,23],</span><br><span class="line">                   [2,3,9]], dtype=tf.float32, shape=[4, 3], name='input_x'</span><br><span class="line">        )</span><br><span class="line">        # 2、创建变量</span><br><span class="line">        weights = tf.constant(</span><br><span class="line">            value=[[-5],</span><br><span class="line">                   [3],</span><br><span class="line">                   [2]], dtype=tf.float32, shape=[3, 1], name='weights'</span><br><span class="line">        )</span><br><span class="line">        bias = tf.constant(</span><br><span class="line">            value=[2], dtype=tf.float32, shape=[1], name='bias'</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 3、构建正向传播过程</span><br><span class="line">        y_hat = tf.matmul(input_x, weights) + bias</span><br><span class="line">        print(y_hat)</span><br><span class="line"></span><br><span class="line">        # 二、构建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            # 执行模型图</span><br><span class="line">            y_hat_ = sess.run(y_hat)</span><br><span class="line">            print(y_hat_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f2():</span><br><span class="line">    """</span><br><span class="line">    变量使用 tf.Variable()来构建</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    # 一、建图</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、创建模型输入</span><br><span class="line">        input_x = tf.constant(</span><br><span class="line">            value=[[1,2,3],</span><br><span class="line">                   [2,3,4],</span><br><span class="line">                   [12,34,23],</span><br><span class="line">                   [2,3,9]], dtype=tf.float32, shape=[4, 3], name='input_x'</span><br><span class="line">        )</span><br><span class="line">        """</span><br><span class="line">        tf.Variable(self,</span><br><span class="line">               initial_value=None,    # 给定初始化的值，可以是python的基本数据类型，也可以是tf的tensor对象</span><br><span class="line">               trainable=True,        # bool 该变量是否参与模型训练。也就是该变量是否会执行梯度下降</span><br><span class="line">               collections=None,</span><br><span class="line">               validate_shape=True,</span><br><span class="line">               caching_device=None,</span><br><span class="line">               name=None,            # tensorflow底层的名字。</span><br><span class="line">               variable_def=None,</span><br><span class="line">               dtype=None,           # 数据类型</span><br><span class="line">               expected_shape=None,</span><br><span class="line">               import_scope=None,</span><br><span class="line">               constraint=None):</span><br><span class="line">        """</span><br><span class="line">        # 2、创建变量</span><br><span class="line">        weights = tf.Variable(</span><br><span class="line">            initial_value=[[-5],</span><br><span class="line">                           [3],</span><br><span class="line">                           [2]], dtype=tf.float32, name='weights'</span><br><span class="line">        )</span><br><span class="line">        print(weights)</span><br><span class="line">        bias_value = tf.constant(</span><br><span class="line">            value=[2], dtype=tf.float32, shape=[1], name='bias'</span><br><span class="line">        )</span><br><span class="line">        bias = tf.Variable(initial_value=bias_value, dtype=tf.float32, name='bias')</span><br><span class="line"></span><br><span class="line">        # 3、构建正向传播过程</span><br><span class="line">        y_hat = tf.matmul(input_x, weights) + bias</span><br><span class="line">        print(y_hat)</span><br><span class="line"></span><br><span class="line">        # 二、构建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            # fixme 执行变量初始化赋值</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line">            # 执行模型图</span><br><span class="line">            y_hat_ = sess.run(y_hat)</span><br><span class="line">            print(y_hat_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f3():</span><br><span class="line">    """</span><br><span class="line">    占位符的使用</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    # 一、建图</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、创建模型输入(占位符)</span><br><span class="line">        # todo 可以使用 shape=[None, 3]， None使用类似于numpy。</span><br><span class="line">        input_x = tf.placeholder(dtype=tf.float32, shape=[None, 3], name='input_x')</span><br><span class="line">        input_c = tf.placeholder_with_default(</span><br><span class="line">            input=1.0, shape=[], name='input_c'</span><br><span class="line">        )</span><br><span class="line">        # 2、创建变量</span><br><span class="line">        weights = tf.Variable(</span><br><span class="line">            initial_value=[[-5],</span><br><span class="line">                           [3],</span><br><span class="line">                           [2]], dtype=tf.float32, name='weights'</span><br><span class="line">        )</span><br><span class="line">        print(weights)</span><br><span class="line">        bias_value = tf.constant(</span><br><span class="line">            value=[2], dtype=tf.float32, shape=[1], name='bias'</span><br><span class="line">        )</span><br><span class="line">        bias = tf.Variable(initial_value=bias_value, dtype=tf.float32, name='bias')</span><br><span class="line"></span><br><span class="line">        # 3、构建正向传播过程</span><br><span class="line">        y_hat = tf.matmul(input_x, weights) + bias</span><br><span class="line">        y_hat1 = y_hat + input_c</span><br><span class="line">        print(y_hat)</span><br><span class="line"></span><br><span class="line">        # 二、构建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            # fixme 执行变量初始化赋值</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line">            # 加载训练数据</span><br><span class="line">            data1 = [[1,2,3],</span><br><span class="line">                   [2,3,4],</span><br><span class="line">                   [12,34,23],</span><br><span class="line">                   [2,3,9]]</span><br><span class="line">            # 执行模型图</span><br><span class="line">            y_hat_, y_hat1_ = sess.run(</span><br><span class="line">                fetches=[y_hat, y_hat1], feed_dict={input_x: data1})</span><br><span class="line">            print(y_hat_, y_hat1_)</span><br><span class="line"></span><br><span class="line">            data2 = [[1, 2, 3],</span><br><span class="line">                     [2, 3, 4],</span><br><span class="line">                     [2, 3, 9]]</span><br><span class="line">            y_hat_, y_hat1_ = sess.run(</span><br><span class="line">                fetches=[y_hat, y_hat1], feed_dict={input_x: data2, input_c: 10.0})</span><br><span class="line">            print(y_hat_, y_hat1_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f4():</span><br><span class="line">    """</span><br><span class="line">    tensorboard的调用</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    # 一、建图</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、创建模型输入(占位符)</span><br><span class="line">        # todo 可以使用 shape=[None, 3]， None使用类似于numpy。</span><br><span class="line">        input_x = tf.placeholder(dtype=tf.float32, shape=[None, 3], name='input_x')</span><br><span class="line">        input_c = tf.placeholder_with_default(</span><br><span class="line">            input=1.0, shape=[], name='input_c'</span><br><span class="line">        )</span><br><span class="line">        # 2、创建变量</span><br><span class="line">        weights = tf.Variable(</span><br><span class="line">            initial_value=[[-5],</span><br><span class="line">                           [3],</span><br><span class="line">                           [2]], dtype=tf.float32, name='weights'</span><br><span class="line">        )</span><br><span class="line">        print(weights)</span><br><span class="line">        bias_value = tf.constant(</span><br><span class="line">            value=[2], dtype=tf.float32, shape=[1], name='bias'</span><br><span class="line">        )</span><br><span class="line">        bias = tf.Variable(initial_value=bias_value, dtype=tf.float32, name='bias')</span><br><span class="line"></span><br><span class="line">        # 3、构建正向传播过程</span><br><span class="line">        y_hat = tf.matmul(input_x, weights) + bias</span><br><span class="line">        y_hat1 = y_hat + input_c</span><br><span class="line">        print(y_hat)</span><br><span class="line"></span><br><span class="line">        # 二、构建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            # fixme 执行变量初始化赋值</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line">            # fixme 加入一段可视化代码</span><br><span class="line">            """</span><br><span class="line">            tf.summary.FileWriter(self,</span><br><span class="line">               logdir,                # 记录日志或者事件的路径。</span><br><span class="line">               graph=None,            # 可视化的图对象</span><br><span class="line">               max_queue=10,</span><br><span class="line">               flush_secs=120,</span><br><span class="line">               graph_def=None,</span><br><span class="line">               filename_suffix=None,</span><br><span class="line">               session=None):</span><br><span class="line">            """</span><br><span class="line">            writer = tf.summary.FileWriter(</span><br><span class="line">                logdir='./model/ai13', graph=sess.graph</span><br><span class="line">            )</span><br><span class="line">            # 加载训练数据</span><br><span class="line">            data1 = [[1,2,3],</span><br><span class="line">                   [2,3,4],</span><br><span class="line">                   [12,34,23],</span><br><span class="line">                   [2,3,9]]</span><br><span class="line">            # 执行模型图</span><br><span class="line">            y_hat_, y_hat1_ = sess.run(</span><br><span class="line">                fetches=[y_hat, y_hat1], feed_dict={input_x: data1})</span><br><span class="line">            print(y_hat_, y_hat1_)</span><br><span class="line"></span><br><span class="line">            data2 = [[1, 2, 3],</span><br><span class="line">                     [2, 3, 4],</span><br><span class="line">                     [2, 3, 9]]</span><br><span class="line">            y_hat_, y_hat1_ = sess.run(</span><br><span class="line">                fetches=[y_hat, y_hat1], feed_dict={input_x: data2, input_c: 10.0})</span><br><span class="line">            print(y_hat_, y_hat1_)</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    f4()</span><br></pre></td></tr></tbody></table></figure></div>
<p>tensorboard 可视化：</p>
<p>打开日志文件 tensorboard —logdir 绝对路径（到文件夹 ）</p>
<p>“”"<br>
tf.summary.FileWriter(self,<br>
logdir,                # 记录日志或者事件的路径。<br>
graph=None,            # 可视化的图对象<br>
max_queue=10,<br>
flush_secs=120,<br>
graph_def=None,<br>
filename_suffix=None,<br>
session=None):<br>
“”"<br>
writer = tf.summary.FileWriter(<br>
logdir=’./model/ai13’, graph=sess.graph<br>
)</p>
<p>TensorFlow控制依赖：</p>
<p>with g.control_dependencies[a,b,c]:</p>
<h1 id="d-and-e-will-only-run-after-a-b-c-have-executed"><a class="markdownIt-Anchor" href="#d-and-e-will-only-run-after-a-b-c-have-executed"></a> “d” and “e” will only run after ‘’’a‘’ ‘’b’’ ‘’c’‘ have executed</h1>
<p>我们可以通过Variable和assign完成变量的定义和更新，但是如果在更新变量之前需要更新其它变量，那么会导致一个比较严重的问题：也就是需要多次调用sess.run方法来进行变量的更新。通过这种方式，代码复杂程度上升，同时也没有执行效率。<br>
解决该问题的方案就是：控制依赖。通过TensorFlow中提供的一组函数来处理不完全依赖的情况下的操作排序问题(即给定哪个操作先执行的问题)， 通过tf.control_dependencies API完成。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def change_var_shape():</span><br><span class="line">    """</span><br><span class="line">    实现动态的更新变量的维度数目。需要设置tf.assign中的validate_shape=False</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、定义一个变量</span><br><span class="line">        x = tf.Variable(</span><br><span class="line">            initial_value=[[0, 2, 3, 4, 0]],</span><br><span class="line">            dtype=tf.float32,</span><br><span class="line">            validate_shape=True   # 设置为False时候，表示不进行初始值shape的验证</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 2、做一个矩阵合并的操作---沿着行进行合并。</span><br><span class="line">        temp = [0.0, 2.0, 3.0, 4.0, 0.0]</span><br><span class="line">        concat = tf.concat(values=[x, tf.expand_dims(temp, axis=0)], axis=0) # 必须沿着已经存在的轴进行拼接。tf.expand_dims扩展轴</span><br><span class="line">        # tf.squeeze()  缩减tensor中维度为1的轴（需要指定）。</span><br><span class="line">        # 3、做一个赋值更新的操作</span><br><span class="line">        assign_opt = tf.assign(ref=x, value=concat, validate_shape=False)</span><br><span class="line"></span><br><span class="line">        # 二、执行会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            for _ in range(5):</span><br><span class="line">                _, x_ = sess.run([assign_opt, x])</span><br><span class="line">                print(x_)</span><br></pre></td></tr></tbody></table></figure></div>
<p><a href="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tens/3.png" data-fancybox="group" data-caption="3" class="fancybox"><img alt="3" data-src="/img/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/tens/3.png" class="lazyload" title="3"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def factorial():</span><br><span class="line">    """</span><br><span class="line">    实现一个求解n阶乘的值，再乘以3的这样一个需求。</span><br><span class="line">    tf.control_dependencies()</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、定义一个占位符，表示一个数字</span><br><span class="line">        input_x = tf.placeholder(tf.float32, shape=None, name='inputx')</span><br><span class="line"></span><br><span class="line">        # 2、定一个变量，作为储存阶乘的值</span><br><span class="line">        sum_x = tf.Variable(initial_value=1.0, dtype=tf.float32, name='sum_x')</span><br><span class="line"></span><br><span class="line">        # 3、执行乘法的操作</span><br><span class="line">        temp = sum_x * input_x</span><br><span class="line">        # 将temp 赋值给 sum_x</span><br><span class="line">        assign_opt = tf.assign(ref=sum_x, value=temp)</span><br><span class="line"></span><br><span class="line">        # 4、将阶乘以后的sum_x 乘以3，得到最终的预测值y_hat</span><br><span class="line">        with tf.control_dependencies(control_inputs=[assign_opt]):</span><br><span class="line">            # fixme 在执行y_hat之前，一定先执行assign_opt赋值的操作</span><br><span class="line">            y_hat = sum_x * 3</span><br><span class="line">        """</span><br><span class="line">        该段代码的含义是：确保执行顺序为  assign_opt --> assign_opt1 --> y_hat</span><br><span class="line">        with tf.control_dependencies(control_inputs=[assign_opt]):</span><br><span class="line">            with tf.control_dependencies(control_inputs=[assign_opt1]):</span><br><span class="line">                # fixme 在执行y_hat之前，一定先执行assign_opt赋值的操作</span><br><span class="line">                y_hat = sum_x * 3</span><br><span class="line">        """</span><br><span class="line"></span><br><span class="line">        # 二、创建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            # 构建迭代累加的值</span><br><span class="line">            data = [1, 3, 5, 7, 9]</span><br><span class="line">            for i in data:</span><br><span class="line">                y_hat_ = sess.run(y_hat, feed_dict={input_x: i})</span><br><span class="line">                print(y_hat_)</span><br></pre></td></tr></tbody></table></figure></div>
<p>TensorFlow设备:</p>
<p>设备是指一块可以用来运算并且拥有自己的地址空间的硬件，如CPU和GPU。Tensorflow为了在执行操作的时候，充分利用计算资源，可以明确指定操作在哪个设备上执行。</p>
<p>一般情况下，不需要显示指定使用CPU还是GPU，TensorFlow会自动检测。如果检测到GPU，TensorFlow会尽可能地利用第一个GPU来执行操作。注意：如果机器上有超过一个可用的GPU，那么除了第一个外其它GPU默认是不参与计算的。所以，在实际TensorFlow编程中，经常需要明确给定使用的CPU和GPU。</p>
<p>“/cpu:0”：表示使用机器CPU运算<br>
“/gpu:0”：表示使用第一个GPU运算，如果有的话<br>
“/gpu:1”：表示使用第二个GPU运算，以此类推</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def factorial():</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        """</span><br><span class="line">        注意事项：</span><br><span class="line">        如果不使用tf.device指定具体运行的设备的话，tensorflow会根据你安装的版本选择默认的设备运行。</span><br><span class="line">            1、如果安装的是cpu版本的tf，那么运行在cpu上面。</span><br><span class="line">            2、如果安装的是gpu版本的tf，那么运算操作一定运行在第一个gpu，但是会在所有gpu上分配内存。</span><br><span class="line">            如何通过tf.device指定了具体的运行设备，那么该运算一定会放到你指定的设备上运行，如果没有，就会报错。</span><br><span class="line">            建议将：allow_soft_placement=True</span><br><span class="line">        """</span><br><span class="line">        with tf.device('/CPU:0'):</span><br><span class="line">            # 1、定义一个占位符，表示一个数字</span><br><span class="line">            input_x = tf.placeholder(tf.float32, shape=None, name='inputx')</span><br><span class="line"></span><br><span class="line">        with tf.device('/CPU:1'):</span><br><span class="line">            # 2、定一个变量，作为储存阶乘的值</span><br><span class="line">            sum_x = tf.Variable(initial_value=1.0, dtype=tf.float32, name='sum_x')</span><br><span class="line"></span><br><span class="line">        with tf.device('/GPU:0'):</span><br><span class="line">            # 3、执行乘法的操作</span><br><span class="line">            temp = sum_x * input_x</span><br><span class="line">            # 将temp 赋值给 sum_x</span><br><span class="line">            assign_opt = tf.assign(ref=sum_x, value=temp)</span><br><span class="line"></span><br><span class="line">            # 4、将阶乘以后的sum_x 乘以3，得到最终的预测值y_hat</span><br><span class="line">            with tf.control_dependencies(control_inputs=[assign_opt]):</span><br><span class="line">                # fixme 在执行y_hat之前，一定先执行assign_opt赋值的操作</span><br><span class="line">                y_hat = sum_x * 3</span><br><span class="line"></span><br><span class="line">        # 二、创建会话</span><br><span class="line">        config = tf.ConfigProto(</span><br><span class="line">            log_device_placement=True, allow_soft_placement=True</span><br><span class="line">        )</span><br><span class="line">        with tf.Session(config=config) as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            # 构建迭代累加的值</span><br><span class="line">            data = [1, 3, 5, 7, 9]</span><br><span class="line">            for i in data:</span><br><span class="line">                y_hat_ = sess.run(y_hat, feed_dict={input_x: i})</span><br><span class="line">                print(y_hat_)</span><br><span class="line"></span><br><span class="line">"""</span><br><span class="line">假设现在有两个GPU，我们代码运行的时候希望仅在第二个GPU上运行，并且仅在第二个GPU上分配内存 </span><br><span class="line">               --> 通过给定环境变量解决</span><br><span class="line">"""</span><br><span class="line"># os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'</span><br><span class="line"># os.environ['CUDA_VISIBLE_DEVICES'] = "0,1"  # 允许当前代码使用第一块、第二块GPU</span><br><span class="line"># os.environ['CUDA_VISIBLE_DEVICES'] = "1"  # 允许当前代码使用第二块GPU</span><br><span class="line"># os.environ['CUDA_VISIBLE_DEVICES'] = "-1"  # 允许当前代码不允许使用GPU</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    factorial()</span><br></pre></td></tr></tbody></table></figure></div>
<p>TensorFlow变量作用域:</p>
<p>通过tf.Variable我们可以创建变量，但是当模型复杂的时候，需要构建大量的变量集，这样会导致我们对于变量管理的复杂性，而且没法共享变量(存在多个相似的变量)。针对这个问题，可以通过TensorFlow提供的变量作用域机制来解决，在构建一个图的时候，就可以非常容易的使用共享命名过的变量。</p>
<p>Tensorflow中有两个作用域，一个是name_scope，另一个是variable_scope。<br>
变量作用域机制在TensorFlow中主要通过两部分组成：<br>
tf.get_variable：通过所给定的名字创建或者返回一个对应的变量<br>
tf.variable_scope：为通过创建的变量或者操作Operation指定命名空间</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># sum_x = 0</span><br><span class="line"># for i in range(1, 5):</span><br><span class="line">#     sum_x += i</span><br><span class="line">#     print(sum_x)</span><br><span class="line">with g.c</span><br><span class="line">def sum1():</span><br><span class="line">    """</span><br><span class="line">    使用tf.assign实现一个累加器。</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、定义一个占位符，表示被累加的值</span><br><span class="line">        input_x = tf.placeholder(tf.float32, shape=None, name='inputx')</span><br><span class="line"></span><br><span class="line">        # 2、定一个变量，作为储存累加的值</span><br><span class="line">        sum_x = tf.Variable(initial_value=0.0, dtype=tf.float32, name='sum_x')</span><br><span class="line"></span><br><span class="line">        # 3、执行累加的操作</span><br><span class="line">        sum_x = sum_x + input_x</span><br><span class="line"></span><br><span class="line">        # 二、创建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            # 构建迭代累加的值</span><br><span class="line">            data = [1, 3, 5, 7, 9]</span><br><span class="line">            for i in data:</span><br><span class="line">                sum_x_ = sess.run(sum_x, feed_dict={input_x: i})</span><br><span class="line">                print(sum_x_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sum2():</span><br><span class="line">    """</span><br><span class="line">    使用tf.assign_add()  或者 tf.assign() 实现一个累加器。</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、定义一个占位符，表示被累加的值</span><br><span class="line">        input_x = tf.placeholder(tf.float32, shape=None, name='inputx')</span><br><span class="line"></span><br><span class="line">        # 2、定一个变量，作为储存累加的值</span><br><span class="line">        sum_x = tf.Variable(initial_value=0.0, dtype=tf.float32, name='sum_x')</span><br><span class="line"></span><br><span class="line">        # 3、执行累加的操作</span><br><span class="line">        # assign_add = tf.assign_add(ref=sum_x, value=input_x)</span><br><span class="line">        """</span><br><span class="line">        tf.assign(ref, value)</span><br><span class="line">            ref: 你要更新的值。</span><br><span class="line">            value: 累加的值。</span><br><span class="line">        """</span><br><span class="line">        temp = sum_x + input_x</span><br><span class="line">        assign_opt = tf.assign(ref=sum_x, value=temp)</span><br><span class="line"></span><br><span class="line">        # 二、创建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            # 构建迭代累加的值</span><br><span class="line">            data = [1, 3, 5, 7, 9]</span><br><span class="line">            for i in data:</span><br><span class="line">                sum_x_, _ = sess.run([sum_x, assign_opt], feed_dict={input_x: i})</span><br><span class="line">                print(sum_x_)</span><br><span class="line"></span><br><span class="line">def change_var_shape():</span><br><span class="line">    """</span><br><span class="line">    实现动态的更新变量的维度数目。需要设置tf.assign中的validate_shape=False</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、定义一个变量</span><br><span class="line">        x = tf.Variable(</span><br><span class="line">            initial_value=[[0, 2, 3, 4, 0]],</span><br><span class="line">            dtype=tf.float32,</span><br><span class="line">            validate_shape=True   # 设置为False时候，表示不进行初始值shape的验证</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # 2、做一个矩阵合并的操作---沿着行进行合并。</span><br><span class="line">        temp = [0.0, 2.0, 3.0, 4.0, 0.0]</span><br><span class="line">        concat = tf.concat(values=[x, tf.expand_dims(temp, axis=0)], axis=0) # 必须沿着已经存在的轴进行拼接。</span><br><span class="line">        # tf.squeeze()  缩减tensor中维度为1的轴（需要指定）。</span><br><span class="line">        # 3、做一个赋值更新的操作</span><br><span class="line">        assign_opt = tf.assign(ref=x, value=concat, validate_shape=False)</span><br><span class="line"></span><br><span class="line">        # 二、执行会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            for _ in range(5):</span><br><span class="line">                _, x_ = sess.run([assign_opt, x])</span><br><span class="line">                print(x_)</span><br><span class="line">with control</span><br><span class="line"></span><br><span class="line">def factorial():</span><br><span class="line">    """</span><br><span class="line">    实现一个求解n阶乘的值，再乘以3的这样一个需求。</span><br><span class="line">    tf.control_dependencies()</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 1、定义一个占位符，表示一个数字</span><br><span class="line">        input_x = tf.placeholder(tf.float32, shape=None, name='inputx')</span><br><span class="line"></span><br><span class="line">        # 2、定一个变量，作为储存阶乘的值</span><br><span class="line">        sum_x = tf.Variable(initial_value=1.0, dtype=tf.float32, name='sum_x')</span><br><span class="line"></span><br><span class="line">        # 3、执行乘法的操作</span><br><span class="line">        temp = sum_x * input_x</span><br><span class="line">        # 将temp 赋值给 sum_x</span><br><span class="line">        assign_opt = tf.assign(ref=sum_x, value=temp)</span><br><span class="line"></span><br><span class="line">        # 4、将阶乘以后的sum_x 乘以3，得到最终的预测值y_hat</span><br><span class="line">        with tf.control_dependencies(control_inputs=[assign_opt]):</span><br><span class="line">            # fixme 在执行y_hat之前，一定先执行assign_opt赋值的操作</span><br><span class="line">            y_hat = sum_x * 3</span><br><span class="line">        """</span><br><span class="line">        该段代码的含义是：确保执行顺序为  assign_opt --> assign_opt1 --> y_hat</span><br><span class="line">        with tf.control_dependencies(control_inputs=[assign_opt]):</span><br><span class="line">            with tf.control_dependencies(control_inputs=[assign_opt1]):</span><br><span class="line">                # fixme 在执行y_hat之前，一定先执行assign_opt赋值的操作</span><br><span class="line">                y_hat = sum_x * 3</span><br><span class="line">        """</span><br><span class="line"></span><br><span class="line">        # 二、创建会话</span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">            # 构建迭代累加的值</span><br><span class="line">            data = [1, 3, 5, 7, 9]</span><br><span class="line">            for i in data:</span><br><span class="line">                y_hat_ = sess.run(y_hat, feed_dict={input_x: i})</span><br><span class="line">                print(y_hat_)</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # sum2()</span><br><span class="line">    # change_var_shape()</span><br><span class="line">    factorial()</span><br></pre></td></tr></tbody></table></figure></div>
<p>TensorFlow变量作用域：</p>
<p>通过tf.Variable我们可以创建变量，但是当模型复杂的时候，需要构建大量的变量集，这样会导致我们对于变量管理的复杂性，而且没法共享变量(存在多个相似的变量)。针对这个问题，可以通过TensorFlow提供的变量作用域机制来解决，在构建一个图的时候，就可以非常容易的使用共享命名过的变量。<br>
Tensorflow中有两个作用域，一个是name_scope，另一个是variable_scope。<br>
变量作用域机制在TensorFlow中主要通过两部分组成：<br>
tf.get_variable：通过所给定的名字创建或者返回一个对应的变量<br>
tf.variable_scope：为通过创建的变量或者操作Operation指定命名空间</p>
<p>tf.get_variable方法在调用的时候，主要需要给定参数名称name，形状shape，数据类型dtype以及初始化方式initializer四个参数。该API底层执行的时候，根据variable score的属性reuse的值决定采用何种方式来获取变量。当reuse值为False的时候(不允许设置)，作用域就是创建新变量设置的，此时要求对应的变量不存在，否则报错；当reuse值为True的时候，作用域就是为重用变量所设置的，此时要求对应的变量必须存在，否则报错。当reuse的值为tf.AUTO_REUSE的时候，表示如果变量存在就重用变量，如果变量不存在，就创建新变量返回。(备注：reuse一般设置在variable score对象上)</p>
<p>tf.get_variable常用的initializer初始化器：</p>
<table>
<thead>
<tr>
<th><strong>初始化器</strong></th>
<th><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>tf.constant_initializer(value)</td>
<td>初始化为给定的常数值value</td>
</tr>
<tr>
<td>tf.random_uniform_initializer(a, b)</td>
<td>初始化为从a到b的均匀分布的随机值</td>
</tr>
<tr>
<td>tf.random_normal_initializer(mean, stddev)</td>
<td>初始化为均值为mean、方差为stddev的服从高斯分布的随机值</td>
</tr>
<tr>
<td>tf.orthogonal_initializer(gini=1.0)</td>
<td>初始化一个正交矩阵，gini参数作用是最终返回的矩阵是随机矩阵乘以gini的结果</td>
</tr>
<tr>
<td>tf.identity_initializer(gini=1.0)</td>
<td>初始化一个单位矩阵，gini参数作用是最终返回的矩阵是随机矩阵乘以gini的结果</td>
</tr>
</tbody>
</table>
<p>tf.variable_score方法的作用就是定义一个作用域，定义在variable_score作用域中的变量和操作，会将variable score的名称作为前缀添加到变量/操作名称前，支持嵌套的作用域，添加前缀规则和文件目录路径的规则类似。<br>
tf.variable_score参数如果给定的是一个已经存在的作用域对象的时候，那么构建变量的时候表示直接跳过当前作用域前缀，直接成为一个完全不同与现在的作用域(直接创建给定作用域下的变量)。但是构建操作的时候，还是和嵌套的方式一样，直接添加子作用域。<br>
tf.variable_score参数中，可以给定当前作用域中默认的初始化器initializer，并且子作用域会直接继承父作用域的相关参数(是否重用、默认初始化器等)</p>
<p>TensorFlow中的name_score和variable_score是两个不同的东西，name_score的主要作用是为op_name前加前缀，variable_score是为get_variable创建的变量的名字加前缀。简单来讲：使用tf.Variable创建的变量受name_score和variable_score的的效果，会给变量添加前缀，但是使用tf.get_variable创建变量只受variable_score的效果。<br>
name_score的主要作用就是：Tensorflow中常常会有数以千计的节点，在可视化的过程中很难一下子展示出来，因此用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。name_scope会影响op_name，不会影响用get_variable()创建的变量，而会影响通过Variable()创建的变量。<br>
注意：variable_score内部会创建一个同名的name_score</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">def f1():</span><br><span class="line">    """</span><br><span class="line">    基于tf.Variable()创建一个变量</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    # 创建一个新的变量，哪怕名字相同。</span><br><span class="line">    w = tf.Variable(</span><br><span class="line">        initial_value=tf.random_normal(shape=[2], mean=0.0, stddev=1.0),</span><br><span class="line">        dtype=tf.float32, name='w'</span><br><span class="line">    )</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def f2(initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0)):</span><br><span class="line">    """</span><br><span class="line">    基于tf.get_variable()获取或者创建变量</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    """</span><br><span class="line">    tf.get_variable(name,        # 变量名字，必须给定</span><br><span class="line">                 shape=None,     # 变量的形状</span><br><span class="line">                 dtype=None,     # 数据类型</span><br><span class="line">                 initializer=None,  # 该变量的初始值生成方式，也就是初始化器。</span><br><span class="line">                 regularizer=None,  # 正则化项</span><br><span class="line">                 trainable=True,    # 变量是否参与模型训练。</span><br><span class="line">                 collections=None,</span><br><span class="line">                 caching_device=None,</span><br><span class="line">                 partitioner=None,</span><br><span class="line">                 validate_shape=True,</span><br><span class="line">                 use_resource=None,</span><br><span class="line">                 custom_getter=None,</span><br><span class="line">                 constraint=None):</span><br><span class="line">        功能：基于给定的name从tensorflow内部获取相应的变量，如果name对应的变量已经存在，那么直接获取该变量，</span><br><span class="line">            如果不存在，直接创建一个新的变量。</span><br><span class="line">        注意：根据name获取变量重用，只支持tf.get_variable创建的变量。</span><br><span class="line">    """</span><br><span class="line">    w = tf.get_variable(</span><br><span class="line">        name='w', shape=[2], dtype=tf.float32, initializer=initializer</span><br><span class="line">    )</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def h1():</span><br><span class="line">    """</span><br><span class="line">    学习tf.get_variable()变量重用用法。</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    # 1、用tf.Variable()创建2个变量</span><br><span class="line">    w11 = f1()</span><br><span class="line">    w12 = f1()</span><br><span class="line"></span><br><span class="line">    # 2、用tf.get_variable()创建变量</span><br><span class="line">    w21 = f2()</span><br><span class="line">    # fixme 需要设置一下，告诉tf名字相同可以重用。</span><br><span class="line">    tf.get_variable_scope().reuse_variables()</span><br><span class="line">    w22 = f2()</span><br><span class="line"></span><br><span class="line">    print(w11.name, w12.name, w21.name, w22.name)</span><br><span class="line">    print('w21 和 w22是同一个变量吗?:{}'.format(w21 == w22))</span><br><span class="line"></span><br><span class="line">    # 二、执行会话</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        print(sess.run([w11, w12, w21, w22]))</span><br><span class="line"></span><br><span class="line">"""</span><br><span class="line">1、学习 变量命名域 tf.varible_scope()用法</span><br><span class="line">2、学习 命名域 tf.name_scope() 用法</span><br><span class="line">3、学习 tf.trainable_variables() 的使用</span><br><span class="line">"""</span><br><span class="line"></span><br><span class="line">"""</span><br><span class="line">tf.variable_scope(self,</span><br><span class="line">               name_or_scope,       # string类型 该变量命名域名字。 </span><br><span class="line">               default_name=None,   # 默认名字。如果参数name_or_scope 为空，那么使用该参数定义的名字。反之，则启用name_or_scope定义的名字</span><br><span class="line">               values=None,         # 传入的值</span><br><span class="line">               initializer=None,    # 变量命名域的初始化器</span><br><span class="line">               regularizer=None,    # 变量命名域的正则化项</span><br><span class="line">               caching_device=None,</span><br><span class="line">               partitioner=None,</span><br><span class="line">               custom_getter=None,</span><br><span class="line">               reuse=None,          # 决定该变量命名域的变量是否重用。</span><br><span class="line">               dtype=None,</span><br><span class="line">               use_resource=None,</span><br><span class="line">               constraint=None,</span><br><span class="line">               auxiliary_name_scope=True):</span><br><span class="line">               </span><br><span class="line">tf.name_scope(</span><br><span class="line">    name,      # 命名域的名字</span><br><span class="line">    default_name=None,  # 命名域默认的名字</span><br><span class="line">    values=None):     # 传入的tensor值。</span><br><span class="line">"""</span><br><span class="line"></span><br><span class="line">def h2():</span><br><span class="line">    with tf.name_scope('ai13'):</span><br><span class="line">        with tf.variable_scope('t1'):</span><br><span class="line">            # 再次创建2个变量</span><br><span class="line">            w11 = f1()</span><br><span class="line">            w12 = f1()</span><br><span class="line"></span><br><span class="line">    # 父域定义的参数 同样也会影响子域。</span><br><span class="line">    with tf.variable_scope('t10', initializer=tf.constant_initializer(28.0)):</span><br><span class="line">        with tf.name_scope('ai14'):  # fixme tf.name_scope对tf.get_variable生成的变量无效</span><br><span class="line">            with tf.variable_scope('t2', reuse=tf.AUTO_REUSE):</span><br><span class="line">                w21 = f2(initializer=None)</span><br><span class="line">                w22 = f2()</span><br><span class="line">    # 做一个加法操作</span><br><span class="line">    with tf.name_scope('ai15'):</span><br><span class="line">        with tf.variable_scope('t5'):</span><br><span class="line">            rezult = tf.add(w11+w12+w21, w22, name='add_rezult')</span><br><span class="line"></span><br><span class="line">    print(w11.name, w12.name)</span><br><span class="line">    print('**'*50)</span><br><span class="line">    print(w21.name, w22.name)</span><br><span class="line">    print('**'*50)</span><br><span class="line">    print(rezult.name)</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        print(sess.run([w11, w12, w21, w22, rezult]))</span><br><span class="line"></span><br><span class="line">        # 基于tensor的名字直接从图中获取对应的tensor的值。</span><br><span class="line">        temp = tf.get_default_graph().get_tensor_by_name('t10/t2/w:0')</span><br><span class="line">        print(sess.run(temp))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def h3():</span><br><span class="line">    """</span><br><span class="line">    学习tf.trainable_variables()使用</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.name_scope('ai13'):</span><br><span class="line">        with tf.variable_scope('t1'):</span><br><span class="line">            # 再次创建2个变量</span><br><span class="line">            w11 = f1()</span><br><span class="line">            w12 = f1()</span><br><span class="line"></span><br><span class="line">    with tf.name_scope('ai14'):  # fixme tf.name_scope对tf.get_variable生成的变量无效</span><br><span class="line">        with tf.variable_scope('t2', reuse=tf.AUTO_REUSE):</span><br><span class="line">            w21 = f2(initializer=None)</span><br><span class="line">            w22 = f2()</span><br><span class="line">    # 做一个加法操作</span><br><span class="line">    with tf.name_scope('ai15'):</span><br><span class="line">        with tf.variable_scope('t5'):</span><br><span class="line">            rezult = tf.add(w11+w12+w21, w22, name='add_rezult')</span><br><span class="line"></span><br><span class="line">    # print(w11.name, w12.name)</span><br><span class="line">    # print('**'*50)</span><br><span class="line">    # print(w21.name, w22.name)</span><br><span class="line">    # print('**'*50)</span><br><span class="line">    # print(rezult.name)</span><br><span class="line">    # fixme 增加一段代码，展示如何获取指定的变量。</span><br><span class="line">    t_vars = tf.trainable_variables()</span><br><span class="line">    print(t_vars)</span><br><span class="line">    ai13_vars = [var for var in t_vars if var.name.startswith('ai13')]</span><br><span class="line">    print(ai13_vars)</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        print(sess.run([w11, w12, w21, w22, rezult]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def h4():</span><br><span class="line">    """</span><br><span class="line">    展示tf.variable_scope() 中values参数的用法</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    default_graph = tf.Graph()</span><br><span class="line">    with default_graph.as_default():</span><br><span class="line">        w12 = f1()</span><br><span class="line"></span><br><span class="line">    graph1 = tf.Graph()</span><br><span class="line">    with graph1.as_default():</span><br><span class="line">        with tf.variable_scope('foo'):</span><br><span class="line">            w21 = f1()</span><br><span class="line"></span><br><span class="line">    graph2 = tf.Graph()</span><br><span class="line">    with graph2.as_default():</span><br><span class="line">        with tf.variable_scope('foo', values=[w12]):</span><br><span class="line">            w22 = f1()</span><br><span class="line"></span><br><span class="line">    print(w21.graph == default_graph)</span><br><span class="line">    print(w22.graph == default_graph)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    h4()</span><br></pre></td></tr></tbody></table></figure></div>
<p>模型持久化</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">"""</span><br><span class="line">模型持久化：就是将训练好的模型（权重 和 网络结构）保存到磁盘中。</span><br><span class="line">    1、可以用于部署。（服务器上训练---拷贝到 移动端进行预测）</span><br><span class="line">    2、进行断点继续训练。（大型模型训练 3--4天）</span><br><span class="line">    3、迁移学习。</span><br><span class="line">"""</span><br><span class="line"></span><br><span class="line"># 变量的持久化</span><br><span class="line">def train():</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 构建2个变量</span><br><span class="line">        v1 = tf.get_variable(</span><br><span class="line">            name='v1', shape=[], dtype=tf.float32,</span><br><span class="line">            initializer=tf.constant_initializer(value=5.0)</span><br><span class="line">        )</span><br><span class="line">        v2 = tf.get_variable(</span><br><span class="line">            name='v2', shape=[], dtype=tf.float32,</span><br><span class="line">            initializer=tf.random_normal_initializer(mean=0.0, stddev=5.0)</span><br><span class="line">        )</span><br><span class="line">        rezult = v1 + v2</span><br><span class="line">        print(rezult)</span><br><span class="line"></span><br><span class="line">        # fixme 1、构建一个持久化的对象</span><br><span class="line">        saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">        # 创建持久化文件路径</span><br><span class="line">        checkpoint_dir = './model/ai13'</span><br><span class="line">        # 创建该路径(不存在该路径的情况下，执行)</span><br><span class="line">        if not os.path.exists(checkpoint_dir):</span><br><span class="line">            # 创建路径</span><br><span class="line">            os.makedirs(checkpoint_dir)</span><br><span class="line">            print('成功创建路径:{}'.format(checkpoint_dir))</span><br><span class="line"></span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line">            print(sess.run([v1, v2, rezult]))</span><br><span class="line">            # [5.0, -4.2127123, 0.7872877]</span><br><span class="line"></span><br><span class="line">            # fixme 触发持久化的操作</span><br><span class="line">            files_name = 'model.ckpt'</span><br><span class="line">            save_files = os.path.join(checkpoint_dir, files_name)</span><br><span class="line">            saver.save(sess=sess, save_path=save_files)  # 执行持久化的</span><br><span class="line">            print('成功将变量持久化到文件：{}'.format(save_files))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def restore_variables():</span><br><span class="line">    """</span><br><span class="line">    从磁盘中恢复变量</span><br><span class="line">    :return:</span><br><span class="line">    """</span><br><span class="line">    with tf.Graph().as_default():</span><br><span class="line">        # 构建2个变量</span><br><span class="line">        v1 = tf.get_variable(</span><br><span class="line">            name='v1', shape=[], dtype=tf.float32,</span><br><span class="line">            initializer=tf.constant_initializer(value=5.0)</span><br><span class="line">        )</span><br><span class="line">        v2 = tf.get_variable(</span><br><span class="line">            name='v2', shape=[], dtype=tf.float32,</span><br><span class="line">            initializer=tf.random_normal_initializer(mean=0.0, stddev=5.0)</span><br><span class="line">        )</span><br><span class="line">        rezult = v1 + v2</span><br><span class="line">        print(rezult)</span><br><span class="line"></span><br><span class="line">        # fixme 1、构建一个持久化的对象</span><br><span class="line">        saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">        # 创建持久化文件路径</span><br><span class="line">        checkpoint_dir = './model/ai13'</span><br><span class="line"></span><br><span class="line">        with tf.Session() as sess:</span><br><span class="line">            # fixme 不需要变量初始化操作了</span><br><span class="line"></span><br><span class="line">            # 做一个恢复模型的操作。</span><br><span class="line">            files_name = 'model.ckpt'</span><br><span class="line">            save_files = os.path.join(checkpoint_dir, files_name)</span><br><span class="line"></span><br><span class="line">            # fixme 直接恢复变量。</span><br><span class="line">            saver.restore(sess=sess, save_path=save_files)</span><br><span class="line">            print(sess.run([v1, v2, rezult]))</span><br><span class="line">            # [5.0, -4.2127123, 0.7872877]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    # train()</span><br><span class="line">    restore_variables()</span><br></pre></td></tr></tbody></table></figure></div></body></html></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Eckle</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://wowli-up.github.io/2020/01/17/tensflow/">https://wowli-up.github.io/2020/01/17/tensflow/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wowli-up.github.io">Eckle的个人网站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/02/03/cnn/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>cnn</span></div></a></div><div class="next-post pull_right"><a href="/2020/01/13/%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A8%A1%E5%9E%8B/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>感知器模型</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'O2mOsDYD5Hx3vwHP4i02Vahz-gzGzoHsz',
  appKey:'kkskDORAC7MO6x0aNw0wMQJp',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/top_img/default.png)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By Eckle</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://wowli-up.github.io/">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-ribbon.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="false"></script><script id="ribbon_piao" mobile="true" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/piao.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script src="https://cdn.jsdelivr.net/npm/activate-power-mode/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true; 
document.body.addEventListener('input', POWERMODE);
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/click_heart.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>